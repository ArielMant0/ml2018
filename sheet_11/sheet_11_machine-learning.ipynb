{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f17c26ef5290d45b564a2185541d945",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f9e7d36d014ba44abbf2fe8283bbe76c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f06f9d6c2384d2f2a11309031ea33b68",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 24, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c989ef972a476392e000cb61e20d673f",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Uncertainty and probability [6 Points]\n",
    "\n",
    "This exercise will focus on concepts introduced in the first part of lecture (ML-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc668602c44bc302af6f47876a0da1d2",
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Modeling uncertainty\n",
    "\n",
    "In the lecture it is claimed that probabilities can summarize several factors:\n",
    "\n",
    "1. missing knowledge\n",
    "1. incapability to devise complete models of complex domains\n",
    "1. chance\n",
    "\n",
    "Think of an example for each of these points and explain how probabilities can be applied in modeling your example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "72f109d9312c8ee012453726ed0c9c61",
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. If I know that 50% of the time, if the grass is wet if the lawn sprinkler was on, then I don't know what cause there may be the other half of the time, but I can still say that the grass is wet due to a lawn sprinkler with a probability of 50%.\n",
    "2. The same.\n",
    "3. Chance can be modelled by probability quite nicely, because probability essentially reflects the behaviour of chance. Think of a six-faced die, it can take any of the values between 1 and 6, each possibility has a probability of $\\frac{1}{6}$, so each possibility accounts for the fact that this event might *not* occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e98ca903dd990da0c0dd014f43b3f280",
     "grade": false,
     "grade_id": "ex1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) Inference by enumeration\n",
    "\n",
    "Given the full joint distribution shown on (ML-11 slide 15), calculate the following:\n",
    "1. $P(\\neg toothache)$\n",
    "1. $P(cavity)$\n",
    "1. $P(toothache \\mid cavity)$\n",
    "1. $P(cavity \\mid toothache \\vee catch)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96c23f82ac2bbe8d6e71640d3430fd6c",
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "|         |  toothache <br> catch| <br>  ¬catch |   ¬toothache <br> catch | <br> ¬catch |\n",
    "|:--------|-------------------------:|--------------------------:|--------------------------:|---------------------------:|\n",
    "| cavity  |                    0.108 |                     0.012 |                     0.072 |                      0.008 |\n",
    "| ¬cavity |                    0.016 |                     0.064 |                     0.144 |                      0.576 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $P(\\neg toothache) = 0.072 + 0.008 + 0.144 + 0.576 = 0.8$\n",
    "2. $P(cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2$\n",
    "3. \n",
    "\\begin{align*}\n",
    "P(toothache~|~cavity) &= \\frac{0.108 + 0.012}{0.2}\\\\\n",
    "&= 0.6\n",
    "\\end{align*}\n",
    "4. \n",
    "\\begin{align*}\n",
    "P(cavity~|~toothache\\lor catch) &= \\frac{P(cavity\\land toothache\\lor catch)}{P(toothache\\lor catch}\\\\\n",
    "&= \\frac{0.108 + 0.012 + 0.072}{0.108 + 0.012 + 0.072 + 0.016 + 0.064 + 0.144}\\\\\n",
    "&\\approx 0.4615\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ce4a21026801118cba8a59edeac6545",
     "grade": false,
     "grade_id": "ex1c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) Conditional probability\n",
    "\n",
    "For each of the following statements, either prove it is true or give a counterexample.\n",
    "1. If P (a | b, c) = P (b | a, c), then P (a | c) = P (b | c)\n",
    "1. If P (a | b, c) = P (a), then P (b | c) = P (b)\n",
    "1. If P (a | b) = P (a), then P (a | b, c) = P (a | c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b305930b2b6a6b420481a5f98bda610",
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "711f158a7b5412d3f8bdf93e090c717f",
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### d) Independence and conditional independence\n",
    "\n",
    "\n",
    "It is quite often useful to consider the effect of some specific propositions in the\n",
    "context of some general background evidence that remains fixed, rather than in the complete\n",
    "absence of information. The following questions ask you to prove more general versions of\n",
    "the product rule and Bayes’ rule, with respect to some background evidence e:\n",
    "\n",
    "1. Prove the conditionalized version of the general product rule:\n",
    "$$P(X, Y \\mid e) = P(X \\mid Y, e)\\cdot P(Y \\mid e) .$$\n",
    "1. Prove the conditionalized version of Bayes’ rule:\n",
    "$$P(Y \\mid X, e) = \\frac{P(X \\mid Y, e)\\cdot P(Y \\mid e)}{P(X \\mid e)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2d82e8ee6329c1cd2884acc53f7c1447",
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcbbb6cd30e796ddab280cf98ed6c95d",
     "grade": false,
     "grade_id": "ex1e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### e) Naive Bayes models\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of\n",
    "categories on the basis of the text it contains. Naive Bayes models are often used for this\n",
    "task. In these models, the query variable is the document category, and the “effect” variables\n",
    "are the presence or absence of each word in the language; the assumption is that words occur\n",
    "independently in documents, with frequencies determined by the document category.\n",
    "1. Explain precisely how such a model can be constructed, given as “training data” a set of documents that have been assigned to categories.\n",
    "1. Explain precisely how to categorize a new document.\n",
    "1. Is the conditional independence assumption reasonable? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f38aa21179d3265a3cf2547518cae962",
     "grade": true,
     "grade_id": "ex1e_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. Given the training data, one can count the appearances of words in correlation to the document type. These numbers can be used to calculate the necessary a prior and conditional probabilities, which can in turn be used to answer queries.\n",
    "2. Given a new document, one counts the occurences of all (key?) words and then weighs the conditional probabilities according to these and then calculates the probability.\n",
    "3. It is not accurate but reasonable in the sense that it is rarely possible to ever really know all dependencies between variables. It is not accurate in the sense that in real life, there are barely any completely independent events (consider medicine, where it is often not possible to say what factor contribute to the development of a disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a1d52b93c81da19b269e13a188b5371",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Bayes networks [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "227c0a791edd9f2b11e01a29959f43e4",
     "grade": false,
     "grade_id": "ex2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Bayes networks\n",
    "\n",
    "Explain in your own words the idea of a Bayes network. How is conditional independence represented in such a network? How can the full joint distribution be regained from such a network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "317483e3c43ea21a9d0b27e8cd2f19db",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "A Bayesian network is a probabilistic graphical model that represents the relationships between events or other observable quantities. Is is represented as a directed acyclic graph, where nodes are the observable quantities that output this quantities probability given an assignment of its parent's variables, and edges indicate conditional dependence (A $\\to$ B means that B depends on a) between two nodes.  \n",
    "\n",
    "Using the chain rule the full joint distribution can be obtained by calculating the product of all conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b56648be1657a979cd035a4522ac1047",
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) Independence in Bayes networks\n",
    "\n",
    "Consider the Bayes network in (ML-11 slide 32):\n",
    "1. If no evidence is observed, are Burglary and Earthquake independent? Prove this from the numerical semantics and from the topological semantics.\n",
    "1. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your answer by calculating whether the probabilities involved satisfy the definition of conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dbdbd8962f13ea2595b59c825f1c9a3b",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. Yes, they are independent.\n",
    "2. No?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "586bd37b4074e3039b48208105d82ced",
     "grade": false,
     "grade_id": "r02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap (part I)\n",
    "\n",
    "This part of the sheet is intended to revise some topics from the lecture, a second part is following on the next sheet. These exercises do not need to be solved in order to qualify for the final exam but it is highly recommended for preparation. Also if you hit any question that should be discussed in more detail, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89444826bba181874e0b85ac48fa4dfd",
     "grade": false,
     "grade_id": "ex01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 1: Concept Learning [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6dd50cc83285c1232c1195d63abffe81",
     "grade": false,
     "grade_id": "ex01a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Concept Learning\n",
    "\n",
    "What is Concept Learning? Is it supervised? Is it local?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f848ccaba8166e9f7484be68c7f298b",
     "grade": true,
     "grade_id": "ex01a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Concept learning means devising a model that has learned a concept like *car* and using that to classifiy whether a given example is an instance of that concept or not (boolean). It is supervised because the correct class/concept of the training data needs to be known during the training process. Concept learning searches through the hypothesis space to find the best fitting hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d737190c7f553a49494e7eb419fb34c",
     "grade": false,
     "grade_id": "ex01b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Find-S\n",
    "Describe the Find-S Algorithm in pseudo code. What is its inductive bias? What are its advantages and drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "18e819151f0265e4150ffa80886a3189",
     "grade": true,
     "grade_id": "ex01b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Pseudo Code:**  \n",
    "```\n",
    "initialize S to the most specific hypothesis (empty)\n",
    "for all training samples x with c(x) = 1:\n",
    "    if S is not consistent with x:\n",
    "        generalize S minimally such that it is consistent with x\n",
    "```\n",
    "        \n",
    "**Inductive Bias:**  \n",
    "Inductive bias is the set of assumptions underlying a learning algorithm, which is used in order to learn (e.g. generalize) from already seen data. Without inductive bias, one cannot learn, because no implications can be drawn from already seen data. The disadvantage of inductive bias is that is dictates *how* an algorithm learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "831be53b6f45c70670bc74332cda5e8a",
     "grade": false,
     "grade_id": "ex01c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Hypotheses space\n",
    "\n",
    "What is the hypotheses space for Candidate-Elimination used in the lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6ee5c5afe2f8e55340f70b98d56d021f",
     "grade": true,
     "grade_id": "ex01c_solution",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hypothesis space for Candidate-Elimination contains all possible hypotheses consisting of conjunctions of literals. It does not contain disjunctive hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5299e31e4c11bbbe108c8acdc632dfa7",
     "grade": false,
     "grade_id": "ex02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 2: Decision Trees [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a02502f7aa7e6b21ac550b63992ee4fb",
     "grade": false,
     "grade_id": "ex02a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Overfitting\n",
    "What is overfitting? How can it be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d3952e567abbde69a5fdffd4f5f3defa",
     "grade": true,
     "grade_id": "ex02a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfitting is the process of training a model such that it learns too much minor details of the training data, leading to worse performance on previously unseen data, because it learns statictically irrelevant features.  \n",
    "\n",
    "It can be avoided by finding a suitable minimum node size for decision trees, pruning the resulting tree or employing the method or *random forests*, which uses several independet trees trained based on random subsets of the actual training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1a0d5e1bc5af31544ce463bd6309c97",
     "grade": false,
     "grade_id": "ex02b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Pruning\n",
    "\n",
    "Name one method for pruning a decision tree and describe it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7cb89deabdf0ceeebcf18306b7b0e929",
     "grade": true,
     "grade_id": "ex02b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Reduced Error Pruning:**  \n",
    "This method removes (leaf?) nodes and assigns the most common class as long as the performance on an independet validation set does not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "581ce55f66d4fa663c711d4f6f8083a2",
     "grade": false,
     "grade_id": "ex02c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Information gain\n",
    "What are entropy and information gain? Provide explanation and formulae. How are they used in ID3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a03b8a2efba5138af0ef701ebc4c8079",
     "grade": true,
     "grade_id": "ex02c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Entropy is a measure of data impurity. Consider an example of a data set, which can be labelled with one of two classes *A* or *B*. When I say that all data belongs to class *A*, but there are data for which that is not the case, then my dataset is somewhat impure, and entropy measures *how* impure a dataset is and can be calculated by:\n",
    "$$\\text{Entropy}(S) = -\\sum_{c\\in S}p_c(S)\\log(p_c(S))$$\n",
    "\n",
    "Information gain describes how much *purer* a dataset becomes if a split for feature *x* is performed, for which it uses the entropy of a dataset. Consider the example below: Let's say my data consists of 5 apples, which each have the two features **expired** and **color**:\n",
    "\n",
    "| Apple | expired | color | tasty |\n",
    "| ----- | ------- | ----- | ----- |\n",
    "|   1   | true    | green | false |\n",
    "|   2   | true    |  red  | false |\n",
    "|   3   | false   | green | true  |\n",
    "|   4   | false   |  red  | true  |\n",
    "|   5   | false   | green | true  |\n",
    "\n",
    "Before I split the data, only 60% of all apples are tasty. If I split the dataset for the feature *color*, then 50% of red apples are tasty, while 66% of green apples are tasty. If I split the dataset for the feature *expired*, then 100% of the **not** expired apples are tasty, while 0% of the expired apples are tasty.  \n",
    "So the second split gives me more information than the first split, because it reduces the entropy of the resulting datasets. In general, information gain is calculated as follows:\n",
    "$$\\text{Information Gain}(S,A) = \\text{Entropy}(S) -\\sum_{v\\in values(A)} \\text{Entropy}(S_v)\\cdot\\frac{|S_v|}{|S|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2523c25781782f0b76adb0bb007f71a9",
     "grade": false,
     "grade_id": "ex03",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 3: Data Mining [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d0854d423cad5f1d191f7fdc7fbf8a54",
     "grade": false,
     "grade_id": "ex03a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Missing values\n",
    "\n",
    "How can you deal with missing values? Name an important algorithm and explain how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "50d5bcf42266796846c068c0305106d1",
     "grade": true,
     "grade_id": "ex03a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Although missing values can also be dealt with using linear regression or simply using the median of the present data, an important algorithm is called **Expectation Maximization**, where the data's underlying distribution is estimated and then values are randomly drawn from that estimated distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e718969e5c226ca2e25eb4ed3aaab07e",
     "grade": false,
     "grade_id": "ex03b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Outliers\n",
    "\n",
    "What are outliers? Can we detect them? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa13ef09df2fc9dfbf0cf5f037441b93",
     "grade": true,
     "grade_id": "ex03b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Outliers are data samples that have extreme values, compared to the rest of the dataset. For example, if I have 200 data samples which contain values between 0 and 1, except for one datum whose value is 42, then that datum is an outlier. Outlierts can occur due to technical reasons (regarding measurement) or because the data naturally has high variation.  \n",
    "Outliers can be detected by calculating their distance from the median $m$ in terms of the standard deviation $\\sigma$:\n",
    "$$z_i = \\frac{|x_i - m|}{\\sigma}$$\n",
    "A datum with a z-value greater than 3.5 is then considered an outlier and removed from the dataset. An algorithm that employs this method is the *Rosner Test*, where outliers are removed iteratively following this scheme:\n",
    "```Python\n",
    "while maximum z > 3.5\n",
    "    maximum z = 0\n",
    "    m = median\n",
    "    s = standard deviation\n",
    "    for x in dataset\n",
    "        z = (x-m)/s\n",
    "        if z > maximum z\n",
    "            maximum z = z\n",
    "    remove x with z = maximum z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e8b28527cd05f7351675347bbf610a7",
     "grade": false,
     "grade_id": "ex03c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) \n",
    "What does the Q-function express in the EM algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "976ec12fd1ac5e884bb64e6967a0ed8c",
     "grade": true,
     "grade_id": "ex03c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It expresses the expected value of the log likelihood function under the current estimate of the parameters $\\theta_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "24ba408a5c6707d5fd9540324b224375",
     "grade": false,
     "grade_id": "ex04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 4: Clustering [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "373adcc0111aa5f139a3bd5b7f74e062",
     "grade": false,
     "grade_id": "ex04a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Clustering\n",
    "\n",
    "Explain the difference between single-linkage and complete-linkage clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0e3f4c5c1a2fce02a2671e99ee34746",
     "grade": true,
     "grade_id": "ex04a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Single- and complete-linkage clustering differ only in the metric they use in order to assign data to clusters. Single-linkage employs the *minimum* cluster distance $D_{min} = \\min_{x\\in X,y\\in Y} d(x,y)$ and complete linkage employs the *maximum* cluster distance $D_{max} = \\max_{x\\in X,y\\in Y} d(x,y)$, where $d(x,y)$ is the euclidean distance function.  \n",
    "\n",
    "Due to these differing cluster distances, single-linkage clustering prefers long thin clusters, where the minimum distance between to clusters is small but the maximum distance may be quite large. On the other hand, complete-linkage clustering prefers compact clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d72601f8cd17c51c838b96fbb3f5d5aa",
     "grade": false,
     "grade_id": "ex04b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Metrics\n",
    "\n",
    "Name three different distance measures and briefly explain them. Check the metric axioms for one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1813d29faac7affb585aa5f0dc0798f5",
     "grade": true,
     "grade_id": "ex04b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "D_{min} &= \\min_{x\\in X,y\\in Y} d(x,y)\\\\\n",
    "D_{max} &= \\max_{x\\in X,y\\in Y} d(x,y)\\\\\n",
    "D_{mean} &= \\frac{1}{|X||Y|}\\sum_{x\\in X,y\\in Y} d(x,y)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Metric Axioms:**  \n",
    "1. $D(x,y) = \\begin{cases} > 0 & \\text{if } x \\neq y\\\\ = 0 & \\text{else}\\end{cases}$\n",
    "2. $D(x,y) = D(y,x)$\n",
    "3. $D(x,y) \\leq D(x,z) + D(z,y)$  \n",
    "\n",
    "$D_{min}$:\n",
    "1. is always given, because the euclidean distance function is 0 for equal values ($x=y$) and greater than 0 otherwise\n",
    "2. is given because symmetry holds for the euclidean distance function\n",
    "3. is given because the triangle inequality holds for the euclidean distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "539b0a8be1e2aeddcca1933b7c3e7f04",
     "grade": false,
     "grade_id": "ex04c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Mixture models\n",
    "\n",
    "What is a mixture model? Explain. Can you provide a formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "718e7afbc96985c166fa509dbc957246",
     "grade": true,
     "grade_id": "ex04c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "K-Means with different gaussians?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3850c1608d8fe96f63823008d11c71b2",
     "grade": false,
     "grade_id": "ex05",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 5: Dimension Reduction [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a033c54e38f456ac44116663f495beea",
     "grade": false,
     "grade_id": "ex05a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Visualization\n",
    "\n",
    "Name three different data visualization techniques to visualize high dimensional data. Explain one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9d23c78e45b73046de3c8529c351df01",
     "grade": true,
     "grade_id": "ex05a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* PCA (principal component analysis)\n",
    "* Scatterplot Matrix\n",
    "* Parallel Plot\n",
    "\n",
    "A scatterplot matrix is a matrix of all combinations of 2 features visualized in a scatterplot each (except for the combinations where $x=y$).\n",
    "![Example Image](http://support.sas.com/documentation/cdl/en/grstatproc/62603/HTML/default/images/gsgscmat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8802c24ea5a82ef6a8542b0dc33de4b2",
     "grade": false,
     "grade_id": "ex05b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) PCA\n",
    "\n",
    "Draw a few data points (ASCII arts or on a sheet of paper) and mark the principal components. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8e43203e4824dae30c77cd382c12812",
     "grade": true,
     "grade_id": "ex05b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Principal Components** are the axes of the n-dimensional ellipsoid which is fit to the data, whose lengths indicate the amount of variance in that dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ba0948130b01d9ab0267c8007d9312d",
     "grade": false,
     "grade_id": "ex05c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Covariance matrix\n",
    "What does a covariance matrix express? How is it computed from data? How is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "90cb090ef14311e4ea69d41da8684682",
     "grade": true,
     "grade_id": "ex05c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A covariance matrix contains the covariance between all pairs of elements of an n-dimensional random variable. Covariance is a measure of the joint variability of two elements, it is positive if both increase simultaneously and negative if one increases while the other decreases.\n",
    "\n",
    "It can be calculated by\n",
    "$$\\text{cov} = E(x-\\mu_x\\cdot y-\\mu_y)$$\n",
    "where $\\mu_x$ is the mean for variable $x$ and $E$ is the expected value (mean).\n",
    "\n",
    "In PCA, the covariance matrix is used to calculate the eigenvector ans eigenvalues from, which are the principaö components used to project the data differently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
