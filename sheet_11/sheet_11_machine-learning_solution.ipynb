{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 24, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Uncertainty and probability [6 Points]\n",
    "\n",
    "This exercise will focus on concepts introduced in the first part of lecture (ML-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Modeling uncertainty\n",
    "\n",
    "In the lecture it is claimed that probabilities can summarize several factors:\n",
    "\n",
    "1. missing knowledge\n",
    "1. incapability to devise complete models of complex domains\n",
    "1. chance\n",
    "\n",
    "Think of an example for each of these points and explain how probabilities can be applied in modeling your example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) Inference by enumeration\n",
    "\n",
    "Given the full joint distribution below, calculate the following:\n",
    "\n",
    "|         |  toothache <br> catch| <br>  ¬catch |   ¬toothache <br> catch | <br> ¬catch |\n",
    "|:--------|-------------------------:|--------------------------:|--------------------------:|---------------------------:|\n",
    "| cavity  |                    0.108 |                     0.012 |                     0.072 |                      0.008 |\n",
    "| ¬cavity |                    0.016 |                     0.064 |                     0.144 |                      0.576 |\n",
    "\n",
    "1. $P(\\neg toothache)$\n",
    "1. $P(cavity)$\n",
    "1. $P(toothache \\mid cavity)$\n",
    "1. $P(cavity \\mid toothache \\vee catch)$\n",
    "\n",
    "If you are familiar with `pandas` you can use the dataframe below to find the solutions. You can of course also write code without using pandas or calculate the answers manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. This asks for the probability that Toothache is true.\n",
    "$$P (toothache ) = 0.108 + 0.012 + 0.016 + 0.064 = 0.2$$\n",
    "1. This asks for the vector of probability values for the random variable Cavity. It has two\n",
    "values, which we list in the order htrue, falsei. First add up $0.108 + 0.012 + 0.072 +\n",
    "0.008 = 0.2$. Then we have\n",
    "$$P(Cavity) = \\langle 0.2, 0.8\\rangle.$$\n",
    "1. This asks for the vector of probability values for Toothache, given that Cavity is true.\n",
    "$$P(Toothache |cavity) = \\langle(.108 + .012)/0.2, (0.072 + 0.008)/0.2\\rangle = \\langle 0.6, 0.4\\rangle$$\n",
    "1. This asks for the vector of probability values for Cavity, given that either Toothache or\n",
    "Catch is true. First compute $P(toothache\\vee catch) = 0.108 + 0.012 + 0.016 + 0.064 +\n",
    "0.072 + 0.144 = 0.416$. Then\n",
    "$$P(Cavity|toothache \\vee catch) =\n",
    "\\langle(0.108 + 0.012 + 0.072)/0.416, (0.016 + 0.064 + 0.144)/0.416\\rangle =\n",
    "\\langle 0.4615, 0.5384 \\rangle$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">toothache</th>\n",
       "      <th colspan=\"2\" halign=\"left\">¬toothache</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>catch</th>\n",
       "      <th>¬catch</th>\n",
       "      <th>catch</th>\n",
       "      <th>¬catch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cavity</th>\n",
       "      <td>0.108</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>¬cavity</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        toothache        ¬toothache       \n",
       "            catch ¬catch      catch ¬catch\n",
       "cavity      0.108  0.012      0.072  0.008\n",
       "¬cavity     0.016  0.064      0.144  0.576"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = pd.MultiIndex.from_product((('toothache', '¬toothache'), ('catch', '¬catch'))) \n",
    "index = ('cavity', '¬cavity')\n",
    "data = [[0.108, 0.012, 0.072, 0.008],\n",
    "        [0.016, 0.064, 0.144, 0.576]]\n",
    "joint_distribution = pd.DataFrame(data, index, columns)\n",
    "joint_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1275cb218b37423f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toothache     0.2\n",
       "¬toothache    0.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "joint_distribution.sum().sum(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03797e9028224cb6",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cavity     0.2\n",
       "¬cavity    0.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.\n",
    "joint_distribution.sum(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c2a23c3fa366ad2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toothache     0.6\n",
       "¬toothache    0.4\n",
       "Name: cavity, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.\n",
    "joint_distribution.loc['cavity', :].sum(level=0) / joint_distribution.loc['cavity', :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2464c36d70ed3b59",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cavity     0.461538\n",
       "¬cavity    0.538462\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. \n",
    "toothache_or_catch = joint_distribution.sum(axis='columns') - joint_distribution.loc[:, ('¬toothache', '¬catch')]\n",
    "toothache_or_catch / toothache_or_catch.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) Conditional probability\n",
    "\n",
    "\n",
    "For each of the following statements, either prove it is true or give a counterexample.\n",
    "1. If P (a | b, c) = P (b | a, c), then P (a | c) = P (b | c)\n",
    "1. If P (a | b, c) = P (a), then P (b | c) = P (b)\n",
    "1. If P (a | b) = P (a), then P (a | b, c) = P (a | c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. True. By the product rule we know P (b, c)P (a|b, c) = P (a, c)P (b|a, c), which by\n",
    "assumption reduces to P (b, c) = P (a, c). Dividing through by P (c) gives the result.\n",
    "1. False. The statement P (a|b, c) = P (a) merely states that a is independent of b and c,\n",
    "it makes no claim regarding the dependence of b and c. A counter-example: a and b\n",
    "record the results of two independent coin flips, and c = b.\n",
    "1. False. While the statement P (a|b) = P (a) implies that a is independent of b, it does\n",
    "not imply that a is conditionally independent of b given c. A counter-example: a and b\n",
    "record the results of two independent coin flips, and c equals the xor of a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### d) Independence and conditional independence\n",
    "\n",
    "\n",
    "It is quite often useful to consider the effect of some specific propositions in the\n",
    "context of some general background evidence that remains fixed, rather than in the complete\n",
    "absence of information. The following questions ask you to prove more general versions of\n",
    "the product rule and Bayes’ rule, with respect to some background evidence e:\n",
    "\n",
    "1. Prove the conditionalized version of the general product rule:\n",
    "$$P(X, Y \\mid e) = P(X \\mid Y, e)\\cdot P(Y \\mid e) .$$\n",
    "1. Prove the conditionalized version of Bayes’ rule:\n",
    "$$P(Y \\mid X, e) = \\frac{P(X \\mid Y, e)\\cdot P(Y \\mid e)}{P(X \\mid e)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The basic axiom to use here is the definition of conditional probability:\n",
    "\n",
    "1. We have\n",
    "$$P(A, B|E) = \\frac{P(A, B, E)}{P(E)}$$\n",
    "and\n",
    "$$P(A|B, E)\\cdot P(B|E) = \\frac{P(A, B, E)}{P(B, E)}\\cdot \\frac{P(B, E)}{P(E)}\n",
    "=\\frac{P(A, B, E)}{P(E)}$$ \n",
    "hence\n",
    "$$P(A, B|E) = P(A|B, E)\\cdot P(B|E)$$\n",
    "1. The derivation here is the same as the derivation of the simple version of Bayes’ Rule. First write down the dual form of the conditionalized product rule, simply by switching A and B in the above derivation:\n",
    "$$P(A, B|E) = P(B|A, E)\\cdot P(A|E)$$\n",
    "Therefore the two right-hand sides are equal:\n",
    "$$P(B|A, E)\\cdot P(A|E) = P(A|B, E)\\cdot P(B|E)$$\n",
    "Dividing through by $P(B|E)$ you get\n",
    "$$P(A|B, E) = \\frac{P(B|A, E)\\cdot P(A|E)}{P(B|E)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex1e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### e) Naive Bayes models\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of\n",
    "categories on the basis of the text it contains. Naive Bayes models are often used for this\n",
    "task. In these models, the query variable is the document category, and the “effect” variables\n",
    "are the presence or absence of each word in the language; the assumption is that words occur\n",
    "independently in documents, with frequencies determined by the document category.\n",
    "1. Explain precisely how such a model can be constructed, given as “training data” a set of documents that have been assigned to categories.\n",
    "1. Explain precisely how to categorize a new document.\n",
    "1. Is the conditional independence assumption reasonable? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1e_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. The model consists of the prior probability $P(Category)$ and the conditional probabilities $P(Word_i|Category)$. For each category $c$, $P(Category = c)$ is estimated as the fraction of all documents that are of category $c$. Similarly, \n",
    "$P(Word_i = true|Category = c)$ is estimated as the fraction of documents of category $c$ that contain $Word_i$.\n",
    "1. Here, every evidence variable $Word_i$ is observed, since we can tell if any given word appears in a given document or not. Hence by Bayes rule we can estimate $P(Category|Word_i)$.\n",
    "1. The independence assumption is clearly violated in practice. For example, the word pair \"machine learning\" occurs more frequently in any given document category than would be suggested by multiplying the probabilities of \"machine\" and \"learning\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Bayes networks [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Bayes networks\n",
    "\n",
    "Explain in your own words the idea of a Bayes network. How is conditional independence represented in such a network? How can the full joint distribution be regained from such a network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) Independence in Bayes networks\n",
    "\n",
    "Consider the Bayes network in (ML-11 slide 32):\n",
    "1. If no evidence is observed, are Burglary and Earthquake independent? Prove this from the numerical semantics and from the topological semantics.\n",
    "1. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your answer by calculating whether the probabilities involved satisfy the definition of conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. Yes. According to (ML-11 slide 33)\n",
    "\\begin{align*}\n",
    "  P(X_1,\\ldots,X_n) = \\prod_{i=1}^n P(X_i\\mid\\textit{Parents}(X))\n",
    "\\end{align*}\n",
    "So numerically one can compute that in our case\n",
    "\\begin{align*}\n",
    "  P(B,E,A,J,M) \n",
    "  & = P(B|\\textit{Parents}(B))\\cdot\n",
    "  P(E|\\textit{Parents}(E))\\cdot\n",
    "  P(A|\\textit{Parents}(A))\\cdot\n",
    "  P(J|\\textit{Parents}(J))\\cdot\n",
    "  P(M|\\textit{Parents}(M)) \\\\\n",
    "  & = P(B)\\cdot\n",
    "  P(E)\\cdot\n",
    "  P(A|B,E)\\cdot\n",
    "  P(J|A)\\cdot\n",
    "  P(M|A)\n",
    "\\end{align*}\n",
    "To get $P(B,E)$ we can apply marginalization:\n",
    "\\begin{align*}\n",
    "  P(B,E,A,J,M) \n",
    "  &= \\sum_{a}\\sum_{j}\\sum_{m} P(B,E,a,j,m) \\\\\n",
    "  &= \\sum_{a}\\sum_{j}\\sum_{m} P(B)\\cdot P(E)\\cdot\n",
    "  P(a|B,E)\\cdot\n",
    "  P(j|a)\\cdot\n",
    "  P(m|a) \\\\\n",
    "  &= \\sum_{a} P(B)\\cdot P(E)\\cdot\n",
    "  P(a|B,E)\\cdot 1 \\cdot 1 \\\\\n",
    "  &= P(B)\\cdot P(E)\\cdot 1\n",
    "\\end{align*}\n",
    "This shows the independence of $B$ and $E$. Topologically $B$ and $E$ are d-separated by $A$ (i.e. any path connecting $B$ and $E$ goes through $A$).\n",
    "1.  We check whether $P(B,E|a) = P(B|a)P(E|a)$. First computing $P(B,E|a)$:\n",
    "\\begin{align} P(B,E|a) = \\alpha P(a|B,E)P(B, E)\n",
    "&= \\alpha\n",
    "\\begin{cases}\n",
    " 0.29 \\cdot 0.001 \\cdot 0.002 & \\text{if $B=b$ and $E=e$} \\\\\n",
    " 0.94 \\cdot 0.001 \\cdot 0.998 & \\text{if $B=b$ and $E=\\neg e$} \\\\\n",
    " 0.29 \\cdot 0.999 \\cdot 0.002 & \\text{if $B=\\neg b$ and $E=e$} \\\\\n",
    " 0.001 \\cdot 0.999 \\cdot 0.998 & \\text{if $B=\\neg b$ and $E=\\neg e$}\n",
    " \\end{cases}\n",
    "\\\\\n",
    "&= \\alpha\n",
    "\\begin{cases}\n",
    " 0.0008 & \\text{if $B=b$ and $E=e$} \\\\\n",
    " 0.3728 & \\text{if $B=b$ and $E=\\neg e$} \\\\\n",
    " 0.2303 & \\text{if $B=\\neg b$ and $E=e$} \\\\\n",
    " 0.3962 & \\text{if $B=\\neg b$ and $E=\\neg e$}\n",
    " \\end{cases}\n",
    "\\end{align}\n",
    "where $\\alpha$ is a normalization constant. Checking $B = \\neg b$ whether $P = (b, e|a) = P (b|a)\\cdot P(e|a)$ we\n",
    "find \n",
    "$$P(b,e|a) = 0.0008\\neq 0.0863 = 0.3736 \\cdot 0.2311 = P(b|a)\\cdot P(e|a)$$\n",
    "showing that $B$ and $E$ are not conditionally independent given $A$.\n",
    "\n",
    "[RN, ex 14.4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap (part I)\n",
    "\n",
    "This part of the sheet is intended to revise some topics from the lecture, a second part is following on the next sheet. These exercises do not need to be solved in order to qualify for the final exam but it is highly recommended for preparation. Also if you hit any question that should be discussed in more detail, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 1: Concept Learning [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex01a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Concept Learning\n",
    "\n",
    "What is Concept Learning? Is it supervised? Is it local?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex01a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Concept learning aims at acquiring knowledge that allows to distinguish exemplars from non exemplars of a given category (concept). It can be formalized as learning a unary predicate $p_c$ on the domain $X$ or equivalently an indicator function $c:X\\to\\{0,1\\}$.\n",
    "\n",
    "Concept learning is usually supervised: the teacher tells the learner if an example falls under the concept or not.\n",
    "\n",
    "As soon as there are is some metric given on the data, there may be local and global concept learners. One may for example use a nearest neighbor learner (local) or a multilayer-perceptron (global) to learn concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex01b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Find-S\n",
    "Describe the Find-S Algorithm in pseudo code. What is its inductive bias? What are its advantages and drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex01b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    1. Initialize $h$ to the most specific hypothesis in H.\n",
    "    2. For each positive training instance x do\n",
    "           For each attribute constraint $a_i$ in h do\n",
    "               If ($a_$i is not satisfied by x) then\n",
    "                   Replace $a_i$ in h by the next more general constraint\n",
    "                     that is satisfied by x.\n",
    "               End if\n",
    "           End for\n",
    "       End for\n",
    "    3. Output h.\n",
    "\n",
    "Inductive Bias: The target concept can be described in its hypothesis space (in our case: it is a conjunction of features). All instances are negative instances unless demonstrated otherwise.\n",
    "\n",
    "Drawback: it does not take negative instances into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex01c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Hypotheses space\n",
    "\n",
    "What is the hypotheses space for Candidate-Elimination used in the lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex01c_solution",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hypothesis space for Candidate-Elimination spreads between the most general and most specific hypotheses. The other hypotheses are made up by conjunction of features which biases the learner and makes it impossible to find a disjunctive solution.\n",
    "\n",
    "The version space on the other hand is a subset of the hypotheses space. It is the set of all hypotheses between and including the general and the specific boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 2: Decision Trees [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex02a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Overfitting\n",
    "What is overfitting? How can it be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex02a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfitting means an overly specific adaptation of the learner to the training data. Not only the general structure of the training data has been learned, but also its specific noise, i.e. artifacts, are learned and hence the learner looses the capability to generalize and work on other data.\n",
    "\n",
    "Overfitting can be detected by using a separate test data set. If the error on the test data increases during training, this indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex02b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Pruning\n",
    "\n",
    "Name one method for pruning a decision tree and describe it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex02b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pruning can be applied to reduce overfitting of a decision tree. Two types of pruning have been introduced in the lecture:\n",
    "\n",
    "*Reduced error pruning:* removes nodes from the decision tree to achieve better generalization on the test set.\n",
    "\n",
    "*Rule based pruning:* translate the decision tree into a set of rules and then prune an individual rule by removing any preconditions that result in improving its accuracy on the\n",
    "validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex02c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Information gain\n",
    "What are entropy and information gain? Provide explanation and formulae. How are they used in ID3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex02c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Entropy measures the inhomogeneity of a data set (the minimal number of bits needed to encode elements from the set) \n",
    "$$E(S) = -p_{+}\\log_2 p_{+} - p_{-}\\log_2p_{-}$$\n",
    "where $p_{+}$ denotes the fraction of positive and $p_{-}$ that of negative examples in the data set. A set $S$ with only positive (or only negative) examples would have no entropy (i.e. $E(S)=0$), while a set with the same number of positive and negative examples has maximal entropy ($E(S)=1$).\n",
    "\n",
    "Information gain is the expected reduction in entropy due to splitting the data set $S$ based on one attribute $A$: denote for every value $v\\in\\operatorname{Values}(A)$ the subset of elements from $S$ where $A=v$ by $S_v$. Then the information gain is given by\n",
    "$$\\operatorname{Gain}(S,A) = E(S) - \\sum_{v\\in\\operatorname{Values}(A)}E(S_v)\\cdot\\frac{|S_v|}{|S|}$$\n",
    "that is, from the entropy of $S$ the entropy values for $S_v$ are subtracted and weighted by their respective sizes. If the subsets $S_v$ are all homogeneous ($E(S_v)=0$), then the information gain is maximal, namely $E(S)$, i.e. the data set can be fully explained by the single attribute $A$. On the other hand, if all $S_v$ have maximal entropy,  no information is gained by splitting based on $A$. In practice, something between these extremes will be the case.\n",
    "\n",
    "ID3 places the node with highest information gain at the root of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex03",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 3: Data Mining [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex03a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Missing values\n",
    "\n",
    "How can you deal with missing values? Name an important algorithm and explain how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex03a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data records with missing values may be simply ignored, or one may try to \"fix\" the record by inserting artificial values into empty slots. The most simple way is to insert just zeros (or some other value) but this will lead to poor data quality. Better approaches try to use statistical properties of the data set to introduce \"natural\" fillers. One approach is to use the mean of the missing attribute, however this ignores possible dependencies between the different attributes. A more sophisticated approach is expectation maximization (EM) to estimate the joint probability distribution of all attributes in an iterative process. Once it is computed, one can use it to determine the most likely value for the missing datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex03b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Outliers\n",
    "\n",
    "What are outliers? Can we detect them? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex03b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An outlier is a value that seemingly does not belong to the rest of the data. It is probably caused by some measurement error (but it may also reflect some real phenomenon).\n",
    "\n",
    "A simple method to detect outliers is to consider their distances from the mean (or median) of the full data set. If this is too large (e.g. greater than 3 standard deviations), the data point is considered to be an outlier (z-test). The Rosner test iteratively removes those outliers until the dataset does not contain anymore of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex03c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) \n",
    "What does the Q-function express in the EM algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex03c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The EM algorithm aims at finding model parameters $\\theta$ that best explain observed data $x$ (which may have missing values $h$). It does so by alternating steps of calculating the expected value of the (log) likelihood function $L(\\theta,x,h)$, using the current estimated parameters $\\theta_t$ (E step), and then finding parameter values $\\theta'$ that maximize this quantity (M step). The $Q$-function expresses the expected likelihood function:\n",
    "$$Q(\\theta\\mid\\theta_{t}) = E_{h\\mid x,\\theta_t}[\\log L(\\theta,x,h)] = \\int P(h\\mid x,\\theta_t)\\cdot \\log P(h\\mid x,\\theta)\\operatorname{d} h+\\log P(x\\mid\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 4: Clustering [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex04a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Clustering\n",
    "\n",
    "Explain the difference between single-linkage and complete-linkage clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex04a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Single-linkage clustering is based on the *minimum distance* that defines the distance between two clusters from the distance of their closest points. Single-linkage clustering tends to chaining.\n",
    "\n",
    "Complete-linkage clustering is based on the *maximum distance* that defines the distance of two clusters to be the maximal distance of two of their points. Complete linkage clustering prefers compact clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex04b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Metrics\n",
    "\n",
    "Name three different distance measures and briefly explain them. Check the metric axioms for one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex04b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Hamming distance: the number of positions where two strings of equal length differ\n",
    "* Chebyshev distance (also: maximum distance): maximal absolute difference in a single coordinate.\n",
    "* p-norm: family of norms, defined by the formula $\\sqrt[p](\\sum_{i=1}^{L}|x_i-y_i|^p)$. Important special cases: city block (aka Manhattan, p=1), euclidean distance (p=2)\n",
    "* Jaccard distance: for binary attributes\n",
    "\n",
    "Metric axioms for Chebyshev distance $d(\\mathbf{x},\\mathbf{y}) := \\max_{i=1,\\ldots,L}|x_i-y_i|$:\n",
    "1. Symmetry: Here we use that the absolute value of the difference is symmetric: $|a-b| = |b-a|$, hence $d(\\mathbf{x},\\mathbf{y}) = \\max_{i=1,\\ldots,L}|x_i-y_i| = \\max_{i=1,\\ldots,L}|y_i-x_i| = d(\\mathbf{y},\\mathbf{x})$\n",
    "2. Coincidence (identity of indiscernibles): $d(\\mathbf{x},\\mathbf{x}) = \\max_{i=1,\\ldots,L}|x_i-x_i| = 0$\n",
    "3. Triangle equation: Here we apply that the triangle inequality holsd for the absolute value of the difference: $|a-c|+|c-b|\\geq|a-b|$, and hence\n",
    "\\begin{align}\n",
    "  d(\\mathbf{x},\\mathbf{z}) + d(\\mathbf{z},\\mathbf{y}) =\n",
    "\\max_{i=1,\\ldots,L}|x_i-z_i| + \\max_{i=1,\\ldots,L}|z_i-y_i| &\n",
    " \\geq \\max_{i=1,\\ldots,L}(|x_i-z_i| + |z_i-y_i|) \\\\\n",
    "& \\geq \\max_{i=1,\\ldots,L}(|x_i-y_i|) = d(\\mathbf{x},\\mathbf{y})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex04c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Mixture models\n",
    "\n",
    "What is a mixture model? Explain. Can you provide a formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex04c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A mixture model describes a two-step process to mix different (simple) data distributions. Such an approach can be used to model a large population with different subpopulations, each which individual characteristics.\n",
    "\n",
    "Formally, one provides a specific distribution $P(X\\mid Z=z)$ for every subpopulation $z$. These are mixed according to the probability $P(Z=z)$ to select an individual from that subpopulation, i.e.\n",
    "$$P(X=x) = \\sum_{z}P(Z=z)\\cdot P(X=x\\mid Z=z)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex05",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 5: Dimension Reduction [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex05a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Visualization\n",
    "\n",
    "Name three different data visualization techniques to visualize high dimensional data. Explain one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex05a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* a *scatterplot matrix* shows 2D projections of the data for all combination of axes\n",
    "* *Chernoff faces:* map parameters to facial features\n",
    "* *parallel coordinates:* map the different data dimensions to different x-coordinates and plot the corresponding values at the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex05b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) PCA\n",
    "\n",
    "Draw a few data points (ASCII arts or on a sheet of paper) and mark the principal components. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex05b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Principal components form a set of linear independent vectors, pointing into the direction of the largest variance. Their length corresponds to the variance in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex05c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Covariance matrix\n",
    "What does a covariance matrix express? How is it computed from data? How is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex05c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The covariance matrix contains the covariance values for all pairs of coordinates. A positive covariance value means that high values for the first coordinate correspond to high values for the second coordinate. A negative covariance value expresses a correspondence of high values in the first coordinate with low values in the second coordinate. A value of $0$ means, that the values of the two coordinates do not correspond to each other.\n",
    "\n",
    "Given a set of $n$ data points in a $d$-dimensional data space as an $n\\times d$-matrix $D$, the covariance matrix is computed as $$C=(D-\\mu)^T\\cdot(D-\\mu)$$ where $\\mu$ denotes the mean vector of the data set.\n",
    "\n",
    "In PCA, the principal components are computed as eigenvectors of the covariance matrix."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
