{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, July 1, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assignment 1: Temporal probability models [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex11a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Hidden Markov Model\n",
    "\n",
    "Explain the structure of a Hidden Markov Model. What probabilities have to be provided for such a model and how can they be specified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex11a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Hidden Markov Model (HMM) explains an observable sequence of data by a sequence of underlying hidden (discrete) states. Each state has certain emission probabilities for the observable variables (sensor model). Furthermore, one has to provide a table of transition probabilities, describing the likelihood of state changes. Finally, one has to provide a prior probability, describing the initial state of the system. In a HMM it is assumed that both, the transition probabilities and the emission probabilities are stationary and fulfill the (first-order) Markov property, allowing for a compact representation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex11b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Inference tasks\n",
    "\n",
    "What is the goal of most likely explanation? Why is the most likely sequence not the sequence of the most likely states? Give an example where these two sequences disagree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex11b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The inference task of finding the most likely explanation is to determine the sequence of hidden states $x_{1:T}$ that is most likely to generate a sequence of observations $e_{1:T}$. If one would compute the most likely state for each time step, one would get distributions over single time steps, whereas to find the most likely sequence we must consider joint probabilities over all the time steps. Given two states $A$ and $B$ are very likely for respective emissions $a$ and $b$, if the transition probability from $A$ to $B$ is 0, no sequence can contain the subsequence $A,B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex11c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Speech recognition\n",
    "\n",
    "Summarize in you own words the approach to speech recognition presented in the lecture. Explain what kind of inference problems occur and how they are solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex11c_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In speech recognition, one aims to reason about a sequence of words (symbols) based on a series of observations (physical signals). This mapping is usually not done directly, but through a chain of more and more abstract representions.\n",
    "1. On the lowest level, the analogous and time continous speech signal is discretized by sampling. This is a rather direct operation, not involving more sophisticated reasoning.\n",
    "1. The resulting sequence is subdivided into overlaping frames for which energy spectra are computed. This form allows for an easy extraction of elementary sound features.\n",
    "1. A phone model explains what features result from a given phone. As a phone has a certain temporal extension, that stretches over more than one feature frame, it is best modeled by some sequential model that takes the phone's internal structure into account, e.g., a Phone HMM. This allows to reconstruct the most likely phone sequence from feature sequence using the Viterbi algorithm.\n",
    "1. A word model combines phone models to describe possible phonetic realizations of a word. So the most likely word can be inferred from a given phone sequence. \n",
    "1. A language model describes the sentences of a language, or in a probabilistic setting, how likely certain sequences of words are in a language. A bigram language model can be combined with a word model to form an HMM. One could use this model to infer the most likely word sequence from the sequence of most likely words, but this will usually result in very poor results. To get better results, one should also include less likely words into the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assignment 2: Implementing HMM [6 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**a)** Implement the basic inference algorithms for HMMs. You may do so by filling in the stubs in the following class. In this implementation, we represent finite probability distributions as one-dimensional numpy arrays, with values summing up to one, e.g. the initial state distribution over three stats $a,b,c$ with $P(a)=0.3, P(b)=0.2, P(c)=0.5$ would be represented by the array `[0.3, 0.2, 0.5]`. Transition matrices are realized by two-dimensional arrays.\n",
    "\n",
    "If you prefer to write your own code, you will find an empty code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e1ef9c034200f6d3",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HMM:\n",
    "    \"\"\"A class implementing a Hidden Markov Model. This class provides methods\n",
    "    to perform the standard inference tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, states, outputs, p_initial, p_transition, p_output):\n",
    "        \"\"\"Create a new HMM.\n",
    "        \n",
    "        Args:\n",
    "            states (list): a list a valid states.\n",
    "            outputs (list): a list of valid output symbols.\n",
    "            p_initial (array_like): initial state distribution. Should add up to one.\n",
    "            p_transition (array_like, ndim=2): state transition probabilities.\n",
    "              Each row is a probability distribution over states, should add up to one.\n",
    "            p_output (array_like, ndim=2): output emission probabilities.\n",
    "              Each row is a probability distribution over output values, should add up to one.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.outputs = outputs\n",
    "        self.p_initial = np.asarray(p_initial)\n",
    "        self.p_transition = np.asarray(p_transition)\n",
    "        self.p_output = np.asarray(p_output)\n",
    "        \n",
    "        # Some sanity checks\n",
    "        assert self.p_initial.shape == (len(self.states),), \"Invalid shape for initial state distribution.\"\n",
    "        assert self.p_initial.sum() == 1.0, \"Initial state probabilities do not add up to one.\"\n",
    "        assert self.p_transition.shape == (len(self.states),len(self.states)), \"Invalid shape for state transition table.\"\n",
    "        assert np.all(np.equal(self.p_transition.sum(axis=1),1.0)), \"State transition probabilities do not add up to one.\"\n",
    "        assert self.p_output.shape == (len(self.states),len(self.outputs)), \"Invalid shape for emission table.\"\n",
    "        assert np.all(np.equal(self.p_output.sum(axis=1),1.0)), \"Emission probabilities do not add up to one.\"\n",
    "\n",
    "    \n",
    "    def prediction(self, p_states):\n",
    "        \"\"\"Compute a prediction step, i.e. from a given\n",
    "        state distribution P(X_{t}), compute the next\n",
    "        state distribution P(X_{t+1}).\n",
    "        \n",
    "        Args:\n",
    "            p_states (ndarray): the current state distribution.\n",
    "            \n",
    "        Retuns:\n",
    "            ndarray: the probability distribution for the next state.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return p_states @ self.p_transition\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def forward(self, p_states, observation):\n",
    "        \"\"\"Compute a forward step, i.e. from a given\n",
    "        state distribution P(X_{t}), and an observation\n",
    "        e_{t+1} compute the next state distribution\n",
    "        P(X_{t+1}).\n",
    "        \n",
    "        Args:\n",
    "            observation: The next observation.\n",
    "            \n",
    "        Retuns:\n",
    "            The probability distribution\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        i = self.outputs.index(observation)\n",
    "        p_trans = p_states @ self.p_transition\n",
    "        p_unnormalized = p_trans * self.p_output[:,i]\n",
    "        return p_unnormalized/p_unnormalized.sum()\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def backward(self, p_observations, observation):\n",
    "        \"\"\"Compute a backward step, i.e. from a given\n",
    "        output distribution P(e_{t+2,T}|X_{t+1}) and an\n",
    "        observation e_{t+1} compute the next previous\n",
    "        output distribution P(e_{t+1,T}|X_{t}).\n",
    "        \n",
    "        Args:\n",
    "            observation: The next observation.\n",
    "            \n",
    "        Retuns:\n",
    "            The probability distribution\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        i = self.outputs.index(observation)\n",
    "        return self.p_output[:,i].T * p_observations @ self.p_transition\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def filtering(self, observations):\n",
    "        \"\"\"Filter this sequence, i.e., iteratively\n",
    "        determine the state probabilities given an \n",
    "        observed output sequence.\n",
    "    \n",
    "        Args:\n",
    "            observations (list): The sequence of observations.\n",
    "\n",
    "        Returns:\n",
    "            list of ndarray: A sequence of state probability\n",
    "            distributions.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        P = [self.p_initial]\n",
    "        for t, s in enumerate(observations):\n",
    "            P.append(self.forward(P[-1], s))\n",
    "\n",
    "        return P\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def smoothing(self, observations, k):\n",
    "        \"\"\"The forward-backward algorithm to determine a state \n",
    "        distribution based on past and future observations.\n",
    "        \n",
    "        Args:\n",
    "            observations (list): The sequence of observations.\n",
    "            k (int): The index for which to determine the state\n",
    "            probabilities.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: the state probability distributions for the\n",
    "            given index k.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        P_forward = self.p_initial\n",
    "        for t, s in enumerate(observations[:k]):\n",
    "            P_forward = self.forward(P_forward, s)\n",
    "\n",
    "        P_backward = np.ones(len(self.outputs))\n",
    "        for t, s in reversed(list(enumerate(observations))[k:]):\n",
    "            P_backward = self.backward(P_backward, s)\n",
    "\n",
    "        p_unnormalized = P_forward * P_backward\n",
    "        return p_unnormalized/p_unnormalized.sum()\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def viterbi(self, observations):\n",
    "        \"\"\"The Viterbi algorithm. Determine the most likely sequence\n",
    "        of hidden states, given an observed output sequence.\n",
    "    \n",
    "        Args:\n",
    "            observations (list): The sequence of observations.\n",
    "\n",
    "        Returns: two return values:\n",
    "            1. list: the most likely sequence of states\n",
    "            2. list of ndarray: A sequence of probability vectors,\n",
    "            providing for each time t and state s the probability\n",
    "            of the most likely initial sequence ending in that state.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        P_max = [self.forward(self.p_initial,observations[0])]\n",
    "        backpointers = []\n",
    "        for t, s in enumerate(observations[1:]):\n",
    "            i = self.outputs.index(s)\n",
    "            p_tmp = P_max[-1] * self.p_transition\n",
    "            P_max.append(p_tmp.max(axis=1) * self.p_output[:,i])\n",
    "            backpointers.append(p_tmp.argmax(axis=1))\n",
    "\n",
    "        sequence = [P_max[-1].argmax()]\n",
    "        for pointer in reversed(backpointers):\n",
    "            sequence.insert(0,pointer[sequence[0]])\n",
    "        ### END SOLUTION\n",
    "\n",
    "        return sequence, P_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-acdc454891a6cfab",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# If you prefer to do your own implementation, place your code here ...\n",
    "### BEGIN SOLUTION\n",
    "# ...\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-075ef0a8accd8b7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**The umbrella example**\n",
    "\n",
    "The following cell initializes a HMM based on the example from the lecture (ML-12 slide 13ff). Use the cells below to check your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values of the hidden states\n",
    "states = ['rain','sun']\n",
    "\n",
    "# The possible output values\n",
    "outputs = [True, False]\n",
    "\n",
    "\n",
    "# The initial distribution of states\n",
    "initial = [0.5, 0.5]\n",
    "\n",
    "# The state transition table\n",
    "transition = [[0.7, 0.3],\n",
    "              [0.3, 0.7]]\n",
    "\n",
    "# The output probabilities for each state\n",
    "output = [[0.9,0.1],\n",
    "          [0.2,0.8]]\n",
    "\n",
    "model = HMM(states, outputs, initial, transition, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the filtering example from the lecture (ML-12 slide 16)\n",
    "\n",
    "observations = [True,True]\n",
    "P = model.filtering(observations)\n",
    "\n",
    "assert np.allclose(P[0], [0.500, 0.500], rtol=5e-2), \"Bad initial distribution for filtering.\"\n",
    "assert np.allclose(P[1], [0.818, 0.182], rtol=5e-2), \"Bad filter values (step 1).\"\n",
    "assert np.allclose(P[2], [0.883, 0.117], rtol=5e-2), \"Bad filter values (step 2).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the smoothing example from the lecture (ML-12 slide 21)\n",
    "\n",
    "observations = [True,True]\n",
    "\n",
    "assert np.allclose(model.smoothing(observations,1), [0.883, 0.117], rtol=5e-2), \"Bad smoothing result (k=1)\"\n",
    "assert np.allclose(model.smoothing(observations,2), [0.883, 0.117], rtol=5e-2), \"Bad smoothing result (k=2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the most likely explanation example from the lecture (ML-12 slide 24)\n",
    "\n",
    "observations = [True,True,False,True,True]\n",
    "most_likely_sequence, P_max = model.viterbi(observations)\n",
    "\n",
    "assert [model.states[s] for s in most_likely_sequence] == ['rain', 'rain', 'sun', 'rain', 'rain'], \"Wrong sequence (Viterbi)\"\n",
    "assert np.allclose(P_max[0], [0.8182, 0.1818], rtol=5e-2), \"Bad viterbi (step 0).\"\n",
    "assert np.allclose(P_max[1], [0.5155, 0.0491], rtol=5e-2), \"Bad viterbi (step 1).\"\n",
    "assert np.allclose(P_max[2], [0.0361, 0.1237], rtol=5e-2), \"Bad viterbi (step 2).\"\n",
    "assert np.allclose(P_max[3], [0.0334, 0.0173], rtol=5e-2), \"Bad viterbi (step 3).\"\n",
    "assert np.allclose(P_max[4], [0.0210, 0.0024], rtol=5e-2), \"Bad viterbi (step 4).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**b)** Now use your implementation to study the behaviour of such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2b1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**1.** Run your model to predict the state distributions without providing any output evidence, i.e. only use the state transition matrix. What do you observe? How does the behavior change if you provide another initial distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2b1-solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2b1-solution-text",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.** Drive your model by providing some observations. Compare the sequence of most likely states with the most likely sequence of states. Can you provide a case where these are different for the \"umbrella\" model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2b2-solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2b2-solution-text",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "recap2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap (part II)\n",
    "\n",
    "This is the second part of the recap material. These exercises do not need to be solved in order to qualify for the final exam but it is highly recommended for preparation. Also if you hit any question that should be discussed in more detail, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r06",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 6: Neural Networks [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r06a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Neural Networks\n",
    "\n",
    "Name three different kinds of Artificial Neural Networks discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r06a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The *multilayer perceptron* (MLP) consists of multiple layers of nodes through which activation is fed forward to compute an output vector to a given input pattern. It usually uses some non-linear activation function in each node and can be trained by a form of error gradient descent called back propagation.\n",
    "\n",
    "* A *radial basis function network* (RBFN) can be considered as a threee layer network: a given input pattern activates the hidden layer using a radial activation function. The output value is then determined as a linear combination of these values. In contrast to the MLP, the RBFN can be considered as a local classifier.\n",
    "\n",
    "* A *self-organizing map* (SOM) is a two layer architecture, in which a high-dimensional input space is connected to a low-dimension grid. The SOM learns a discretized, low dimension representation of the input data. In contrast to MLP and RBFN, the SOM is an unsupervised approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r06b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Backpropagation\n",
    "\n",
    "Which of the following formulae describes the backpropagation of the error through hidden layers in a Multilayer Perceptron?\n",
    "Assume they are calculated for each $k=L_H \\dots 1$ and $i=1\\dots N(k)$.\n",
    "\n",
    "1. $\\delta_i(k) = f^\\prime(o_i(k)) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1, k)o_j(k)$\n",
    "2. $\\delta_i(k) = f^\\prime(o_i(k)) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1, k)\\delta_j(k+1)$\n",
    "3. $\\delta_i(k) = f^\\prime(o_i(k)) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k, k-1)\\delta_j(k+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r06b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Formula 1 uses the output instead of the deltas.\n",
    "* Formula 2 is correct. \n",
    "* Formula 3 uses the wrong weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r06c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Hebb's rule\n",
    "Explain Hebb's rule. Provide a formula. What is the relation to Oja's rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r06c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea of Hebb's rule is to strengthen connections between neurons that fire simultanously. This is expressed by the formula\n",
    "$$\\Delta w_i = \\varepsilon y(\\vec{x}\\cdot\\vec{w})\\cdot x_i$$\n",
    "A high activation value $y(\\vec{x}\\cdot\\vec{w})$ coinciding with high values in the input $x_i$ results in a strong adaptation. \n",
    "\n",
    "Oja's rule is a modification of the standard Hebb Rule. It addresses the problem that the simple form of Hebb's rule does not allow the connection weights to decrease, so that they will eventually become arbitrarily large. Oja's rule avoids this problem by introducing some multiplicative normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r07",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 7: Local Methods [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r07a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Local methods\n",
    "\n",
    "What are differences between local and global methods? What are advantages or disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r07a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A model is termed local, if the adaptation of model parameters only has local effects, i.e. it will only effect a subset of input values, located close to each other in the input space. In contrast, changing a parameter of a global model may effect all input values. Hence, local methods are considered to be more robust during training, as single (faulty) traning examples only effect a part of the system. Furthermore, such methods may be better to manage, as the effect of a single parameter is easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r07b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) MLP and RBFN\n",
    "\n",
    "Is an MLP or are RBFN local methods? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r07b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RBFN are a local method as each hidden neuron has a local area of responsibility. In contrast, a MLP is global, as changing a single weight may change the input-output mapping for all input patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r07c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c)  Nearest neighbor\n",
    "\n",
    "How does the nearest neighbor approach work? How can it be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r07c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In nearest neighbor learning, all training examples are stored in memory. Upon inference, when a new input is given, the most similar training example (the nearest neighbor) is retrieved and used to provide the result. In the $k$-nearest neighbor approach, not only one, but $k$ nearest neighbors are retrieved, and the result is determined by averaging over these values. A more advanced version uses a weighted average, including the distance of the neighbors from the given data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r08",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 8: Classification [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r08a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Classfier\n",
    "\n",
    "What is a classifier? What is the relation to a concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r08a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A classifier assigns a class to an entity based on its attributes (attributes might be color, height, weight, shape, ..., classes might be car, house, person, banana, yes, ...). Formally, a classifier is a function $c:X\\to C$ that assigns a class $c(x)\\in C$ to every object $x\\in X$. Hence, a concept is a special classifier with only two classes $C=\\{\\operatorname{true},\\operatorname{false}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r08b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Comparison of classifiers\n",
    "\n",
    "Name three different classifiers and compare them. Think about biases and assumptions, separatrices, sensitivity, locality, parameters and speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r08b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Usually assuming 0 mean and only binary problems!*\n",
    "\n",
    "| Classifier           | Biases and Assumptions | Separatrices | Sensitivity | Locality | Parameters | Speed |\n",
    "|----------------------|------------------------|--------------|-------------|----------|------------|-------|\n",
    "| Euclidean classifier | voronoi tesselation around class centers | linear | sensitive to far outliers | global | none | very fast |\n",
    "| Linear discriminant analysis | normally distributed data with equal covariances | linear | sensitive to far outliers | global | none | very fast |\n",
    "| Quadratic classifier (e.g. QDA) | ? | conic: e.g. hyperbola, parabola, ellipsis, line | sensitive to outliers | global | none | fast |\n",
    "| Polynom classifier | ? | almost arbitrary | overfitting for high degrees | global | polynomial degree | fast |\n",
    "| Nearest neighbor classifier  | classification for neighbors are similar | implicit: neighbors (voronoi cells around training data) | distance function | local | number of neighbors $k$ | $\\mathcal{O}(N)$ (instant training, linear classification) |\n",
    "| Bayesian classifier | expected cost is minimized | discriminate functions (probability distributions) | overlapping classifications (only probabilities), noise is modeled | global | none | varies (underlying data and method for discriminate functions, see ML-09 Slides 5f) |\n",
    "| MLP (not necessarily binary) | smooth interpolation | almost arbitrary | noise sensitive | global | activation functions, learning rate | slow |\n",
    "| RBFN (not necessarily binary) | locality in data/clusters | ellipses/circular | robust to noise | local | regions of responsibility,  learning rate | comparably slow |\n",
    "| SVM | mercer's condition, input mapping, kernel function | high dimensional hyperplane, nonlinear in data space | handles noise with slacking variables | global | none | efficient |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex08c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) SVM\n",
    "\n",
    "What is a support vector? How does the kernel trick work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r08c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a two part dataset, the support vectors are those vectors of each class, that are closest to vectors from the other class. Then the separatrix is computed as the hyperplane with maximal distance to these support vectors.\n",
    "\n",
    "In many cases, two classes can not be separated by a simple hyperplane. However, often one can find an embedding of the data into a higher-dimensional space where it becomes linearly separable. The kernel trick uses the fact, that for many tasks one does not have to compute the embedding explicitly, but it suffices to be able to compute the inner product of embedded datapoints, using an appropriate *kernel function*. This trick is most prominently used in support vector based classification, but it can also be used for other tasks like clustering and PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r09",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 9: Reinforcement Learning [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r09a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a)\n",
    "\n",
    "What is an agent in terms of reinforcement learning? Name an example of an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex09a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An agent has sensors and can perform actions in a specified environment. (See PEAS: Performance, Environment, Actuators, Sensors)\n",
    "\n",
    "Some possible agents are: mobile robots or game AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r09b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b)\n",
    "\n",
    "What is the Markov assumption? How is it related to Q-learning? Give an example for which it does not hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r09b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The (first-order) Markov assumption means that state $s_{t+1}$ only depends on its predecessor state $s_t$ and the action $a_t$ performed then, i.e.: $s_{t+1} = \\delta(s_t, a_t)$. This allows to specify a $Q$-function of the form $Q(s_t,a_t)$, instead of $Q(s_0,a_0,\\ldots,s_t,a_t)$. The Markov assumption does not hold in situations where more information is needed than provided by the previous state. For example for sentence parsing with each word being a state the Markov assumption does not hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r09c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) \n",
    "What does the $Q$-function express in reinforcement learning and how is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r09c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $Q$-function provides the (discounted) maximal cummulative reward for performing an action $a$ in state $s$. The $Q$-function can be stated in a recursive form as\n",
    "$$Q(s,a) = r(s,a) + \\gamma\\cdot\\max_{a'\\in\\operatorname{Actions}(s')}{Q(s',a')}.$$\n",
    "with $\\gamma$ being the discount factor.\n",
    "The $Q$-function can be used to define an optimal action selection policy for a reinforcement learning problem by\n",
    "$$a^{\\ast}(s) = \\operatorname{argmax}_{a\\in\\operatorname{Actions}(s)}Q(s,a)$$\n",
    "In $Q$-learning the $Q$-function is approximated by an iterative procedure.\n",
    "\n",
    "Remark: There is no relation of the $Q$-function of reinforcement learning and the $Q$-function in the EM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 10: Modeling Uncertainty [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "r10a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Uncertainty\n",
    "\n",
    "Why do we need to model uncertainties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r10a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Uncertainty can occur due to many reasons:\n",
    "* sparse data\n",
    "* unreliable data (noise)\n",
    "* uncertain outcomes\n",
    "* high complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex10b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Naive Bayes\n",
    "\n",
    "What is a naive Bayes classifier? Why is it naive? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "r10b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Naive Bayes is a probabilistic classifier, i.e. a classifier that instead of a class assignment $x\\mapsto c(x)\\in C$ provides a probability value $P(C=c\\mid X=x)$. The naive Bayes classifier applies Bayes theorem to compute the posterior (diagnostical) probability $P(C\\mid X)$ from likelihood values $P(X\\mid C)$ (and prior $P(X)$) that have been learned from training data. Naivity refers to the simplifying assumption that the different features $X_1,\\ldots,X_n$ are conditional independent, given the class $C$, an assumption that may not be true in general but proves to work well in many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "r10c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Probabilities\n",
    "\n",
    "Given the following table, calculate the probability of drawing a blue candy blindly (assume the bags are chosen equally likely). Then calculate the probability that our drawn blue candy was drawn from the red bag.\n",
    "\n",
    "|                | blue candies | green candies |\n",
    "|----------------|--------------|---------------|\n",
    "| **red bag**    |            5 |            10 |\n",
    "| **yellow bag** |           20 |            10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex10c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(C=b) = \\frac{1}{2} \\left( \\frac{5}{15} + \\frac{20}{30} \\right) = \\frac{1}{2}$$\n",
    "$$P(B=r|C=b) = \\frac{P(C=b|B=r)P(B=r)}{P(C=b)} = \\frac{ \\frac{5}{15} \\frac{1}{2} }{ \\frac{1}{2} } = \\frac{5}{15} = \\frac{1}{3}$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
