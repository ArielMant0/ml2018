{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "60019695a75826e426e31bb0a36ae88d",
     "grade": false,
     "grade_id": "cell-a26d5efc968151ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2017) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e81575aaa58c46d7d86ae35d368cdd4",
     "grade": false,
     "grade_id": "cell-932f957b5e17c264",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 02: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edd3b1762dfdbc1780d925dd0144969f",
     "grade": false,
     "grade_id": "cell-a85c8970c448755a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "By now everyone should have found a group. If someone still has none but wants to participate in the course please contact one of the tutors.\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, April 22, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whom ever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfa7e093820395b90cff4f77942ac78",
     "grade": false,
     "grade_id": "math-euclid",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Euclidean Space) [2 Bonus Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet.\n",
    "It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them.\n",
    "Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session.\n",
    "Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "899ac70855dbd336b6edef96c4c1a6f5",
     "grade": false,
     "grade_id": "math-euclid-q1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a *Euclidean space*? What is the *Cartesian plane*? How are they usually denoted? How to write points in these spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "10da800d32483f0d4bfa9cf43624b3c0",
     "grade": true,
     "grade_id": "math-euclid-a1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Euclidean space may refer to different things, but is often used for the vector space $\\mathbb{R}^n$. The Cartesian plane is the 2-dimensional Euclidean space $\\mathbb{R}^2$, which is parametrized by coordinates in respect to the two perpendicular axes (usually denoted X and Y).\n",
    "A point in the Cartesian plane is written as $(x,y)$ where $x,y \\in \\mathbb{R}$, while a point in Euclidean space is written as an n-dimensonal vector $x = (p_1, p_2, \\dots, p_n)$ where $p_i \\in \\mathbb{R} ~\\forall ~1 \\leq i \\leq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0bf071a4c6d1ff977bcd336eb96d7daa",
     "grade": false,
     "grade_id": "math-euclid-q2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the *norm* of a vector in a Euclidean space? How to *add* and *substract* two vectors? How is the *Euclidean distance* defined? Are there other ways to measure distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4f95335b2228d0766f6bdbbba83f857d",
     "grade": true,
     "grade_id": "math-euclid-a2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "A norm of a vector in Euclidean space is a function that assigns a strictly positive value to a vector, representing its length or size. The Euclidean norm is defined by\n",
    "$$||\\vec{x}||_2 := \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}$$\n",
    "The addition of two vectors is defined as\n",
    "$$\\vec{x} + \\vec{y} := (x_1 + y_1, x_2 + y_2, \\dots, x_n + y_n)$$\n",
    "and the subtraction is similarly defined as\n",
    "$$\\vec{x} - \\vec{y} := (x_1 - y_1, x_2 - y_2, \\dots, x_n - y_n)$$\n",
    "There is not just one norm but several, like the p-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7a7304a941a4864f1c0ad2cce2d86b3",
     "grade": false,
     "grade_id": "math-euclid-q3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the (standard) *scalar product* of two vectors? How is it related to the length and angle between these vectors? Name some use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e5c2f82430f4ba30219c7b0e9a43836d",
     "grade": true,
     "grade_id": "math-euclid-a3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The standard scalar product of two vectors is a function that assigns a scalar value to two vectors, also called the canonical scalar product. It is given by\n",
    "$$\\langle\\vec{x}, \\vec{y}\\rangle := \\sum_{i=1}^n x_iy_i$$\n",
    "The result is 0 when the two vectors are perpendicular to each other. Taking the square root of the standard scalar product of a vector with itself is taking the Euclidean norm of that vector, which calculates its length. The angle between two vectors can also be calculated using the scalar product, because\n",
    "$$\\langle\\vec{x}, \\vec{y}\\rangle \\iff ||\\vec{x}||~||\\vec{y}|| \\cos(\\theta)$$\n",
    "where $\\theta$ is the angle between x and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33cb9b299bd68d89dc69155cca58ae7b",
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Decision Trees [2 Points]\n",
    "Draw the decision trees for the following boolean functions. Either use pen and paper and bring the results to the feedback session or employ your ASCII artist within below. \n",
    "\n",
    "Note: $\\oplus := xor$, that means one of the operands has to be true, while the other one has to be false:\n",
    "\n",
    "$\\oplus$ | $B$ | $\\neg B$\n",
    "---------|-----|---------\n",
    "$A$      |  f  |    t\n",
    "$\\neg A$ |  t  |    f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79a5db9b1525a51878c831a9d5fe5b35",
     "grade": false,
     "grade_id": "1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) $\\neg A \\wedge B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3eacc13c6841e84a9e15f429370fa40",
     "grade": true,
     "grade_id": "1a_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                       +\n",
    "                       |\n",
    "            A=F        |       A=T\n",
    "          +-------------------------+\n",
    "          |                         |\n",
    "     B=T  |   B=F                   |\n",
    "    +------------+                  +\n",
    "    |            |                  F\n",
    "    |            |           \n",
    "    +            +          \n",
    "    T            F       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f500739a15568445866c4b555d7a6e36",
     "grade": false,
     "grade_id": "1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) $A \\oplus B$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "081b38d16bbf73acc91330546dd4a73e",
     "grade": true,
     "grade_id": "1b_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                       +\n",
    "                       |\n",
    "            A=F        |       A=T\n",
    "          +-------------------------+\n",
    "          |                         |\n",
    "     B=T  |   B=F             B=T   |   B=F\n",
    "    +------------+           +-------------+\n",
    "    |            |           |             |\n",
    "    |            |           |             |\n",
    "    +            +           +             +\n",
    "    T            F           F             T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2e623bde4f83e6d64bc12abbdc8cbfc",
     "grade": false,
     "grade_id": "1c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) $A \\vee (B \\wedge C) \\vee (\\neg C \\wedge D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93daf8f9de9e7669bbd6994deb2f6f03",
     "grade": true,
     "grade_id": "1c_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                  +\n",
    "                  |\n",
    "       A=T        |       A=F\n",
    "     +-------------------------+\n",
    "     |                         |\n",
    "     |                   B=T   |         B=F\n",
    "     T                 +-------------------------+\n",
    "                       |                         |\n",
    "                  C=T  | C=F                 C=T |  C=F\n",
    "               +--------------+           +--------------+\n",
    "               |              |           |              |\n",
    "               |         D=T  |  D=F      |         D=T  |   D=F\n",
    "               T        +-----------+     F       +---------------+    \n",
    "                        |           |             |               |\n",
    "                        |           |             |               |\n",
    "                        T           F             T               F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "74cd6378a22605b0e3e960e3ceb0d3ef",
     "grade": false,
     "grade_id": "1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### d) $(A \\rightarrow (B \\wedge \\neg C)) \\vee (A \\wedge B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "13ffc3b5266121e14c1f8d82f95f2f5d",
     "grade": true,
     "grade_id": "1d_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                  +\n",
    "                  |\n",
    "       A=F        |       A=T\n",
    "     +-------------------------+\n",
    "     |                         |\n",
    "     |                   B=T   |   B=F\n",
    "     T                 +-----------------+\n",
    "                       |                 |\n",
    "                       |                 |\n",
    "                       T                 F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b60d7db6c515794ed215753fa117334f",
     "grade": false,
     "grade_id": "2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Entropy and Information Gain [8 Points]\n",
    "\n",
    "In many machine learning applications it is crucial to determine which criterions are necessary for a good classification. Decision trees have those criterions close to the root, imposing an order from significant to less significant criterions. One way to select the most important criterion is to compare its information gain or its entropy to others. The following dataset is a hands-on example for this method. \n",
    "\n",
    "Consider the following attributes with their possible values:\n",
    "\n",
    "  * $raining = \\{yes, no\\}$\n",
    "  * $tired = \\{yes, no\\}$\n",
    "  * $late = \\{yes, no\\}$\n",
    "  * $distance = \\{short, medium, long\\}$\n",
    "\n",
    "And a training data set consisting of those attributes:\n",
    "\n",
    "| #  | raining | tired | late | distance | attend_party |\n",
    "|----|---------|-------|------|----------|--------------|\n",
    "| 1  | yes     | no    | no   | short    | **yes**      |\n",
    "| 2  | yes     | no    | yes  | medium   | **no**       |\n",
    "| 3  | no      | yes   | no   | long     | **no**       |\n",
    "| 4  | yes     | yes   | yes  | short    | **no**       |\n",
    "| 5  | yes     | no    | no   | short    | **yes**      |\n",
    "| 6  | no      | no    | no   | medium   | **yes**      |\n",
    "| 7  | no      | yes   | no   | long     | **no**       |\n",
    "| 8  | yes     | no    | yes  | short    | **no**       |\n",
    "| 9  | yes     | yes   | no   | short    | **yes**      |\n",
    "| 10 | no      | yes   | no   | medium   | **no**       |\n",
    "| 11 | no      | yes   | no   | long     | **no**       |\n",
    "| 12 | no      | yes   | yes  | short    | **no**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d15839ac8d68a301dc4dc8449bc07a5b",
     "grade": false,
     "grade_id": "2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a)\n",
    "\n",
    "Build the root node of a decision tree from the training samples given in the table above by calculating the information gain for all four attributes (raining, tired, late, distance).\n",
    "\n",
    "$$\\operatorname{Gain}(S,A) = \\operatorname{Entropy}(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|}\\operatorname{Entropy}(S_v)$$\n",
    "\n",
    "$$\\operatorname{Entropy}(S) = -p_{\\oplus} log_{2} p_{\\oplus} - p_{\\ominus} log_{2} p_{\\ominus}$$\n",
    "\n",
    "$S$ is the set of all data samples. $S_v$ is the subset for which attribute $A$ has value $v$. An example for attribute **tired** with value $yes$ would be:\n",
    "$$|S_{yes}| = 7, S_{yes}:[1+, 6‚àí]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "374b2672465f2f683716faee14ae8d12",
     "grade": true,
     "grade_id": "2a_answer",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The first node splits for the values of the attribute **late**, because it's information gain is the highest of all four attributes.\n",
    "\n",
    "Gain(S,raining)  = 0.093  \n",
    "Gain(S,tired)    = 0.169  \n",
    "Gain(S,late)     = 0.251  \n",
    "Gain(S,distance) = 0.189  \n",
    "\n",
    "So the resulting root node and branches look like this:\n",
    "\n",
    "           S = [+4,-8]\n",
    "           E(S) = 0.918\n",
    "           ------------\n",
    "           |   late   |\n",
    "           ------------\n",
    "                |\n",
    "         yes    |      no\n",
    "      +---------------------+\n",
    "      |                     |\n",
    "      |                     |\n",
    "      +                     +\n",
    "    S = [+0,-4]           S = [+4,-4]\n",
    "    E(S) = 0              E(S) = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "564babda2996cead4eacceca3b3eeb64",
     "grade": false,
     "grade_id": "2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b)\n",
    "\n",
    "Perform the same calculation as in **a)** but use the gain ratio instead of the information gain. Does the result for the root node change?\n",
    "\n",
    "$$\\operatorname{GainRatio}(S,A) = \\frac{\\operatorname{Gain}(S,A)}{\\operatorname{SplitInformation}(S,A)}$$\n",
    "\n",
    "$$\\operatorname{SplitInformation}(S,A) = - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} \\log_{2} \\frac{|S_{v}|}{|S|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d0e7f2c03813216bf62ab5f0acab6a77",
     "grade": true,
     "grade_id": "2b_answer",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "GainRatio(S,raining)  = 0.093  \n",
    "GainRatio(S,tired)    = 0.172  \n",
    "GainRatio(S,late)     = 0.273  \n",
    "GainRatio(S,distance) = 0.126\n",
    "\n",
    "The root node stays the same, although the values changed a little for every feature except for *raining*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d72fbdf1b31b6681ea8bb9d053979f6",
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: ID3 algorithm [5 Points]\n",
    "\n",
    "Implement the following two functions in Python. Take a look at the `assert`s to see how the function should behave. An assert is a condition that your function is required to pass. Most of the conditions here are taken from the lecture slides (ML-03, Slide 12 & 13). Don't worry if you do not get all asserts to pass, just comment the failing ones out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a6c9e8564c72fc8093b7245bf3a9c652",
     "grade": false,
     "grade_id": "3a_info",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Entropy\n",
    "\n",
    "$$\\operatorname{Entropy}(S) = - \\sum_{i=1...c} p_i log_2 p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ef2554da1e5f6b6a815a13e1c9f6912f",
     "grade": false,
     "grade_id": "3a_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def entropy(s):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a given target value set. \n",
    "    \n",
    "    Args:\n",
    "        s (list): Target classes for specific observations.\n",
    "        \n",
    "    Returns:\n",
    "        The entropy of s.\n",
    "    \"\"\"\n",
    "    prob = {}\n",
    "    for value in s:\n",
    "        if not value in prob:\n",
    "            prob[value] = 1\n",
    "        else:\n",
    "            prob[value] = prob[value]+1\n",
    "            \n",
    "    size = len(s)\n",
    "    return -sum(map(lambda kv: (kv[1]/size) * log2(kv[1]/size), prob.items()))\n",
    "# See ML-03, Slide 12 & 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "120a1ed08a6d679643e859dd5ad08243",
     "grade": true,
     "grade_id": "3a_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert entropy([1,1,1,0,0,0]) == 1.0\n",
    "assert round(entropy([1,1,1,1,0,0,0]), 3) == 0.985\n",
    "assert round(entropy([1,1,1,1,1,1,0]), 3) == 0.592\n",
    "assert round(entropy([1,1,1,1,1,1,0,0]), 3) == 0.811\n",
    "assert round(entropy([2,2,1,1,0,0]), 3) == 1.585\n",
    "assert round(entropy([2,2,2,1,0]), 3) == 1.371\n",
    "assert round(entropy([2,2,2,0,0]), 3) == 0.971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5e8b62cf831972afa2ba4f73d8c70c53",
     "grade": false,
     "grade_id": "3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b)  Information Gain\n",
    "\n",
    "$$\\operatorname{Gain}(S,A) = \\operatorname{Entropy}(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} \\operatorname{Entropy}(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ebdc9b685ade2b882cd033ecc36c44ce",
     "grade": false,
     "grade_id": "3b_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def helper(s, attr_values, value):\n",
    "    sv = []\n",
    "    for tar, val in zip(s, attr_values):\n",
    "        if val == value:\n",
    "            sv.append(tar)\n",
    "     \n",
    "    return sv\n",
    "\n",
    "def inner_gain(sv):\n",
    "    return entropy(sv) * len(sv)\n",
    "\n",
    "def gain(targets, attr_values):\n",
    "    \"\"\"\n",
    "    Calculates the expected reduction in entropy due to sorting on A.\n",
    "    \n",
    "    Args:\n",
    "        targets (list): Target classes for observations in attr_values.\n",
    "        attr_values (list): Values of each instance for the respective attribute.\n",
    "        \n",
    "    Returns:\n",
    "        The information gain of \n",
    "    \"\"\"\n",
    "    entropy_s = entropy(targets)\n",
    "    size_s = len(targets)\n",
    "    vals = set(attr_values)\n",
    "    \n",
    "    return entropy_s - sum([inner_gain(helper(targets, attr_values, attr)) / size_s for attr in vals])\n",
    "\n",
    "# ss = [1,0,0,0,1,1,0,0,1,0,0,0]\n",
    "# print('raining =', round(gain(ss, [1,1,0,1,1,0,0,1,1,0,0,0]),3))\n",
    "# print('tired =', round(gain(ss, [0,0,1,1,0,0,1,0,1,1,1,1]),3))\n",
    "# print('late =', round(gain(ss, [0,1,0,1,0,0,0,1,0,0,0,1]),3))\n",
    "# print('distance =', round(gain(ss, [0,1,2,0,0,1,2,0,0,1,2,0]),3))\n",
    "\n",
    "# Dont' mind this, just checking...\n",
    "\n",
    "def inner_gain_ratio(sv, len_s):\n",
    "    return len(sv)/len_s * log2(len(sv)/len_s)\n",
    "\n",
    "def split_information(targets, attr_values):\n",
    "    vals = set(attr_values)\n",
    "    size_s = len(targets)\n",
    "       \n",
    "    return -sum([inner_gain_ratio(helper(targets, attr_values, attr), size_s) for attr in vals])\n",
    "\n",
    "def gain_ratio(targets, attr_values):\n",
    "    return gain(targets, attr_values) / split_information(targets, attr_values)\n",
    "\n",
    "# ss = [1,0,0,0,1,1,0,0,1,0,0,0]\n",
    "# print('raining =', round(gain_ratio(ss, [1,1,0,1,1,0,0,1,1,0,0,0]),3))\n",
    "# print('tired =', round(gain_ratio(ss, [0,0,1,1,0,0,1,0,1,1,1,1]),3))\n",
    "# print('late =', round(gain_ratio(ss, [0,1,0,1,0,0,0,1,0,0,0,1]),3))\n",
    "# print('distance =', round(gain_ratio(ss, [0,1,2,0,0,1,2,0,0,1,2,0]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97b42b047c444b4d3c65aee8c1d44384",
     "grade": true,
     "grade_id": "3b_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_S_ = [0,0,1,1,1,0,1,0,1,1,1,1,1,0]\n",
    "assert round(gain(assert_S_, [1,1,1,1,0,0,0,1,0,0,0,1,0,1]), 3) == 0.152\n",
    "assert round(gain(assert_S_, [0,1,0,0,0,1,1,0,0,0,1,1,0,1]), 3) == 0.048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cd6e9dca6eb953989e6f7ef7a6ad37c",
     "grade": false,
     "grade_id": "3c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) ID3\n",
    "\n",
    "In the next two cells we have implemented the ID3 algorithm following the pseudocode from [Wikipedia](https://en.wikipedia.org/wiki/ID3_algorithm#Pseudocode) - it relies on your two functions from above, `entropy` and `gain`. Try to understand what the code does and replace `YOUR CODE HERE` with meaningful comments describing the respective parts of the code. Though its often annoying, being able to read other peoples code is one of the key skills (and obstacle) in software engineering. So give it a try! Otherwise you are of course welcome to write your own implementation.\n",
    "\n",
    "Below the algorithm's cell, it is applied to two data sets. Run those and discuss the differences. For which data set is the ID3 algorithm better suited and why? (Enter your answer in the cell below the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d4b63221966b12245e22fb99093c1e78",
     "grade": true,
     "grade_id": "3c_answer",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "\n",
    "\n",
    "class Node(namedtuple('Node', 'label children')):\n",
    "    \"\"\"\n",
    "    A small node representation with a pretty string representation.\n",
    "    \"\"\"\n",
    "    def __str__(self, level=0):\n",
    "        return_str ='{}{!s}\\n'.format(' ' * level * 4, self.label)\n",
    "        for child in self.children:\n",
    "            return_str += child.__str__(level + 1)\n",
    "        return return_str\n",
    "\n",
    "def id3(examples, attributes, target_attribute = None): \n",
    "    \"\"\"\n",
    "    Calculate a tree of Nodes (fields: label [string], children [list]) \n",
    "    using the ID3 algorithm found as pseudocode on Wikipedia.\n",
    "    \"\"\"\n",
    "    # If everything is classified uniformly return a node with a that label\n",
    "    if all(target == examples['targets'][0] for target in examples['targets']):\n",
    "        return Node('Result: {!s}'.format(examples['target_names'][examples['targets'][0]]), [])\n",
    "    \n",
    "    # If there are no more attributes to consider, then return a node whose label is that of\n",
    "    # the most commonly found attribute of the example list\n",
    "    if len(attributes) == 0:\n",
    "        attr = Counter(data_sample[target_attribute] for data_sample in examples['data']).most_common(1)\n",
    "        return Node('Attribute: {!s}, {!s} occurences'.format(examples['attributes'][target_attribute], attr), [])\n",
    "    \n",
    "    # Calculate the gain for every attribute that has not been considered yet\n",
    "    gains = [gain(examples['targets'], [r[attribute] for r in examples['data']]) \n",
    "             for attribute in attributes]\n",
    "    # Store the index of the attribute with the highest information gain\n",
    "    max_gain_attribute = attributes[gains.index(max(gains))]\n",
    "    \n",
    "    # Construct a new node using the most informative attribute\n",
    "    root = Node('Attribute: {!s} (gain {!s})'.format(examples['attributes'][max_gain_attribute], \n",
    "                                                     round(max(gains), 4)), [])\n",
    "    \n",
    "    # Append a new node for every possible attribute value of the 'max gain attribute' (branching)\n",
    "    for vi in set(data_sample[max_gain_attribute] for data_sample in examples['data']):\n",
    "        # Construct a new node with attribute value 'vi'\n",
    "        child = Node('Value: {!s}'.format(vi), [])\n",
    "        root.children.append(child)\n",
    "        \n",
    "        # Store the indices of all data samples where the 'max gain attribute' has\n",
    "        # the value 'vi' and then put those samples into new data structures \n",
    "        # containing the sample data and their target values \n",
    "        vi_indices = [idx for idx, data_sample in enumerate(examples['data']) \n",
    "                          if data_sample[max_gain_attribute] == vi]\n",
    "        examples_vi = dict(examples)\n",
    "        examples_vi['data'] = [examples['data'][i] for i in vi_indices]\n",
    "        examples_vi['targets'] = [examples['targets'][i] for i in vi_indices]\n",
    "        \n",
    "        if examples_vi['data']:\n",
    "            # If the list of examples where the 'max gain attribute' has value 'vi' is not\n",
    "            # empty, make a recursive call of the algorithm with the reduced examples and\n",
    "            # without this attribute, because we won't need to branch for it again\n",
    "            child.children.append(\n",
    "                id3(examples_vi,\n",
    "                    [attribute_ for attribute_ in attributes if not attribute_ == max_gain_attribute],\n",
    "                    max_gain_attribute)\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            # If the list of examples where the 'max gain attribute' has value 'vi' is\n",
    "            # empty, return a node whose label is that of the most commonly found attribute of the example list\n",
    "            attr = Counter(examples_vi['targets']).most_common(1)\n",
    "            label = 'Attribute: {!s}, {!s} occurences'.format(examples['attributes'][target_attribute], attr)\n",
    "            child.children.append(Node(label, []))\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the ID3 algorithm on the party data set which you already know from assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: late (gain 0.2516)\n",
      "    Value: yes\n",
      "        Result: no\n",
      "    Value: no\n",
      "        Attribute: distance (gain 0.75)\n",
      "            Value: short\n",
      "                Result: yes\n",
      "            Value: medium\n",
      "                Attribute: tired (gain 1.0)\n",
      "                    Value: yes\n",
      "                        Result: no\n",
      "                    Value: no\n",
      "                        Result: yes\n",
      "            Value: long\n",
      "                Result: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('party.json', 'r') as party_file:\n",
    "    party = json.load(party_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "assert round(gain(party['targets'], [r[2] for r in party['data']]), 3) == 0.252\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_party = id3(party, list(range(len(party['attributes']))))\n",
    "\n",
    "print(tree_party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the ID3 algorithm on the famous iris flowser data set, which you will hear more about in assignment 4 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: petal length (gain 1.4463)\n",
      "    Value: 1.2\n",
      "        Result: Iris-setosa\n",
      "    Value: 4.0\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.6\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.5\n",
      "        Attribute: sepal length (gain 0.5436)\n",
      "            Value: 5.6\n",
      "                Result: Iris-versicolor\n",
      "            Value: 4.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.2\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.4\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.0\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.4\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.7\n",
      "                Result: Iris-versicolor\n",
      "    Value: 6.1\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.4\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.7\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.3\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.2\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.9\n",
      "        Attribute: sepal width (gain 0.971)\n",
      "            Value: 2.5\n",
      "                Result: Iris-versicolor\n",
      "            Value: 2.7\n",
      "                Result: Iris-virginica\n",
      "            Value: 3.1\n",
      "                Result: Iris-versicolor\n",
      "            Value: 3.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 2.8\n",
      "                Result: Iris-virginica\n",
      "    Value: 5.6\n",
      "        Result: Iris-virginica\n",
      "    Value: 1.7\n",
      "        Result: Iris-setosa\n",
      "    Value: 5.3\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.2\n",
      "        Result: Iris-versicolor\n",
      "    Value: 1.9\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.1\n",
      "        Result: Iris-setosa\n",
      "    Value: 3.6\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.1\n",
      "        Attribute: sepal length (gain 0.5436)\n",
      "            Value: 5.8\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.5\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.3\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.9\n",
      "                Result: Iris-virginica\n",
      "    Value: 3.0\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.8\n",
      "        Attribute: sepal length (gain 1.0)\n",
      "            Value: 6.8\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.2\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.9\n",
      "                Result: Iris-versicolor\n",
      "    Value: 3.7\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.6\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.5\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.7\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.8\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.4\n",
      "        Result: Iris-virginica\n",
      "    Value: 6.9\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.7\n",
      "        Result: Iris-versicolor\n",
      "    Value: 1.0\n",
      "        Result: Iris-setosa\n",
      "    Value: 3.3\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.4\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.9\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.9\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.3\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.8\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.0\n",
      "        Attribute: sepal length (gain 0.8113)\n",
      "            Value: 6.3\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.7\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.7\n",
      "                Result: Iris-versicolor\n",
      "    Value: 1.3\n",
      "        Result: Iris-setosa\n",
      "    Value: 6.0\n",
      "        Result: Iris-virginica\n",
      "    Value: 1.5\n",
      "        Result: Iris-setosa\n",
      "    Value: 3.5\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.1\n",
      "        Result: Iris-versicolor\n",
      "    Value: 1.6\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.4\n",
      "        Result: Iris-setosa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('iris.json', 'r') as iris_file:\n",
    "    iris = json.load(iris_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "assert round(gain(iris['targets'], [r[2] for r in iris['data']]), 3) == 1.446\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_iris = id3(iris, list(range(len(iris['attributes']))))\n",
    "\n",
    "print(tree_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3a9118e68b062cd5d8433ef6e12d3437",
     "grade": true,
     "grade_id": "3d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The resulting tree for the iris data set has a much higher branching factor than the tree for the party data set, but it is a bit shorter than. This means that with little information, we can find the right flower fast, however, the rules that can be derived from this tree are quite a lot.  \n",
    "In contrast, much less rules are derived from the party tree, and although the tree itself is taller, it is actually more general than the other tree. This is due to the continuous nature of the possible values for the petal length attribute, which may result in the tree not generalizing as well for other examples with slightly different values, while the party tree only works on discrete values (or classes) and therefore might generalize better for other data samples of the same problem. Consider the case when the petal length of another example varies by 0.003. This might not be correctly classified by the tree, because it does not **exactly** meet the criteria of the rules.\n",
    "\n",
    "This assessment could change when employing ID3 extensions that can handle continuous data better (as was done below). Then, we only have a binary tree, which is more readable and can produce better rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Decision Trees on Iris Flowers [5 Points]\n",
    "\n",
    "In this exercise we are going to examine and compare two decision trees that were generated from the iris flower data set ([Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)) where three variations of the iris flower are quantified. The Iris data set is a classical example of a labeled dataset, i.e. every sample consists of two parts: features and labels. There are four features per sample in this data set (sepal length ($x_1$), sepal width ($x_2$), petal length ($x_2$) and petal width ($x_4$) in cm) and a corresponding label (Iris Setosa, Iris Versicolour, Iris Virginica). These samples are by nature **noisy**, no matter how carefully the measurement was taken - slight deviation from the actual length **cannot be avoided**. We want to learn how the features are related to the label so that we could (in the future) predict the label of a new sample automatically. One way to obtain such a `classifier` is to train a decision tree on the data.\n",
    "\n",
    "Here are two decisions tree generated by the data set. We will now take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 1:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                     x3 < 4.95 |   x3 >= 4.95      +\n",
    "                        +--------------+       virginica\n",
    "                        |              |\n",
    "                        |              |\n",
    "              x4 < 1.65 | x4 >= 1.65   +\n",
    "                 +------------+    virginica\n",
    "                 |            |\n",
    "                 |            |\n",
    "                 +            +\n",
    "            versicolor    virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                               +                   +\n",
    "                          versicolor           virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "What does it mean that the features $x1$ and $x2$ do not appear in the decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2eecbf907d099f392a93c6c70500246e",
     "grade": true,
     "grade_id": "4a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "It means that they do not contribue to the classifier, e.g. that they do not hold information relevant for classifying a data sample. This is the case when the actual value of a feature does not impact the classification. Take for example the case that I have a decision tree that tells me whether I like a certain coffee depending on wheter it contains milk and sugar. If I always like coffee with milk, *regardless* of whether it also contains sugar or not, there is no point in distinguishing between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "With which method from the lecture might the second tree have been generated from the first one? Explain the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa53969cbbf7c079a6ef4850d3cebda3",
     "grade": true,
     "grade_id": "4b",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The second tree might be a result of *Reduced Error Pruning*. This procedure prunes subtrees by replacing them with nodes that hold the most common classifier of that subtree. This is only done as long as accuracy performance on the validation set does not decrease. This produces a shorter tree by removing nodes that were produced by noise (otherwise performance would have dropped on the validation set too).\n",
    "  \n",
    "The second tree might however **also** be the result of *Rule Post Pruning*. Rule Post Pruning is a procedure employed to reduce overfitting. It does so converting every path from the root to a leaf into a rule (e.g. A=1 and B=2 then TRUE). From these rules, antecedents are removed (e.g. A=1) as long as the resulting rule improves the accuracy of the tree on the validation set. When that is finished, the rules are sorted by their accuracy on the validation set and applied in that order.\n",
    "\n",
    "Either is possible, because both produce a smaller, more general tree from a given decision tree. Which one was actually used can only be determined when looking at the training and validation data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "After training the tree we can calculate the accuracy, i.e. the percentage of the training set that is classified correctly. Although the first tree was trained on the data set until no improvement of the accuracy was possible, its accuracy is *only* 98%. Explain why it is not 100 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e5382e843bc42ad7daa61898f5db7e95",
     "grade": true,
     "grade_id": "4c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "This can be explained due to noise or errors in the training data. If no rules can be formulated that fit all training samples, then the tree cannot achieve perfect accuracy.  \n",
    "\n",
    "This argument is further supported by the fact that Wikipedia states that there are two known errors in the iris flower data set. =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Tree 2 only has a 96% accuracy on the training set. Why might this tree still be preferable over tree 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c528275b25812856398b43be4465852",
     "grade": true,
     "grade_id": "4d",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The second tree is much shorter than the first one, meaning it is more general and therefore (hello Occam's Razor) preferrable. Also, accuracy in the *training data* is not the overall best accuracy measurement. Generating a tree that fits the training data *too* well might be due to **overfitting**. This means that the tree is too specialized and performes worse on data *other* than the training data.  \n",
    "\n",
    "Therefore it can be better to have a tree with slightly worse accuracy on the training data, if the accuarcy on the validation data (and other data in general) is better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
